{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOWp+GBVa65lcXZ1E5VHdNe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yVn1ZwBz94MA","executionInfo":{"status":"ok","timestamp":1716445698911,"user_tz":-540,"elapsed":4531,"user":{"displayName":"정채원","userId":"16403372842423495263"}},"outputId":"5a425a91-f4e3-4f99-caf7-e8f807e5c750"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","첫번째 입력:  버거킹 바삭한 신메뉴 3000원도 안되는 가격\n","첫번째 one-hot 출력:  [0. 1. 0.]\n","첫번째 토큰 결과: [1, 2804, 41, 5168, 1512, 23]\n","학습셋 제목 최대 길이 : 14\n","테스트셋 제목 최대 길이 : 11\n","첫번째 패딩 토큰: [   0    0    0    0    0    0    0    0    1 2804   41 5168 1512   23]\n","Word Size: 12464\n"]}],"source":["import numpy\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from keras.utils import to_categorical\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing import sequence\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Embedding\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","# seed 값 설정\n","numpy.random.seed(3)\n","tf.random.set_seed(3)\n","\n","# 데이터 읽어오기\n","train_data = pd.read_csv(\"/content/drive/MyDrive/AI/week11/train_mydataset_6000.csv\", header=0, names=['title', 'label'])\n","X_train = train_data['title']\n","Y_train = train_data['label']\n","test_data = pd.read_csv(\"/content/drive/MyDrive/AI/week11/test_mydataset_1500.csv\", header=0)\n","X_test = test_data['title']\n","Y_test = test_data['label']\n","\n","# 데이터 확인하기\n","print(\"첫번째 입력: \", X_train[0])\n","Y_train_onehot = to_categorical(Y_train)\n","Y_test_onehot = to_categorical(Y_test)\n","print(\"첫번째 one-hot 출력: \", Y_train_onehot[0])\n","\n","# 토큰화\n","token = Tokenizer()\n","token.fit_on_texts(X_train)\n","X_train_sequence = token.texts_to_sequences(X_train) # 2차원 배열\n","X_test_sequence = token.texts_to_sequences(X_test) # 2차원 배열\n","print(\"첫번째 토큰 결과:\", X_train_sequence[0])\n","\n","# 최대 길이 구하기\n","max_len_train = max(len(l) for l in X_train_sequence)\n","print(\"학습셋 제목 최대 길이 :\", max_len_train)\n","\n","max_len_test = max(len(l) for l in X_test_sequence)\n","print(\"테스트셋 제목 최대 길이 :\", max_len_test)\n","\n","# 패딩, 서로 다른 길이의 데이터를 4로 맞춤\n","padded_X_train = pad_sequences(X_train_sequence, max_len_train)\n","padded_X_test = pad_sequences(X_test_sequence, max_len_train)\n","print(\"첫번째 패딩 토큰:\", padded_X_train[0])\n","\n","# 임베딩에 입력될 단어 수 지정\n","word_size = len(token.word_index) + 1\n","print(\"Word Size:\", word_size)"]},{"cell_type":"code","source":["# 단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과 출력\n","model = Sequential()\n","model.add(Embedding(word_size, 8, input_length=max_len_train))\n","model.add(Flatten())\n","model.add(Dense(3, activation='softmax'))\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.fit(padded_X_train, Y_train_onehot, epochs=20)\n","\n","print(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_X_test, Y_test_onehot)[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8WoiRLZGVWZ","executionInfo":{"status":"ok","timestamp":1716445810469,"user_tz":-540,"elapsed":41857,"user":{"displayName":"정채원","userId":"16403372842423495263"}},"outputId":"d55d9aa9-6b67-4d28-91d7-9394c53e3b79"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","188/188 [==============================] - 5s 26ms/step - loss: 0.8938 - accuracy: 0.6548\n","Epoch 2/20\n","188/188 [==============================] - 2s 9ms/step - loss: 0.6654 - accuracy: 0.7200\n","Epoch 3/20\n","188/188 [==============================] - 1s 4ms/step - loss: 0.4148 - accuracy: 0.8460\n","Epoch 4/20\n","188/188 [==============================] - 1s 5ms/step - loss: 0.2383 - accuracy: 0.9462\n","Epoch 5/20\n","188/188 [==============================] - 1s 4ms/step - loss: 0.1393 - accuracy: 0.9790\n","Epoch 6/20\n","188/188 [==============================] - 1s 7ms/step - loss: 0.0862 - accuracy: 0.9917\n","Epoch 7/20\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0569 - accuracy: 0.9963\n","Epoch 8/20\n","188/188 [==============================] - 1s 5ms/step - loss: 0.0394 - accuracy: 0.9973\n","Epoch 9/20\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0286 - accuracy: 0.9985\n","Epoch 10/20\n","188/188 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9993\n","Epoch 11/20\n","188/188 [==============================] - 1s 3ms/step - loss: 0.0166 - accuracy: 0.9993\n","Epoch 12/20\n","188/188 [==============================] - 1s 3ms/step - loss: 0.0131 - accuracy: 0.9993\n","Epoch 13/20\n","188/188 [==============================] - 1s 4ms/step - loss: 0.0105 - accuracy: 0.9997\n","Epoch 14/20\n","188/188 [==============================] - 1s 3ms/step - loss: 0.0086 - accuracy: 0.9998\n","Epoch 15/20\n","188/188 [==============================] - 1s 3ms/step - loss: 0.0071 - accuracy: 0.9997\n","Epoch 16/20\n","188/188 [==============================] - 1s 3ms/step - loss: 0.0059 - accuracy: 1.0000\n","Epoch 17/20\n","188/188 [==============================] - 1s 4ms/step - loss: 0.0050 - accuracy: 1.0000\n","Epoch 18/20\n","188/188 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 19/20\n","188/188 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 1.0000\n","Epoch 20/20\n","188/188 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 1.0000\n","47/47 [==============================] - 0s 2ms/step - loss: 0.4109 - accuracy: 0.8613\n","\n"," Accuracy: 0.8613\n"]}]}]}